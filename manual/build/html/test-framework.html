
<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.5.3. Testing framework &#8212; Manual</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/default.css?v=bb97339c" />
    <script src="_static/documentation_options.js?v=316584e3"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4.5.4. How to write tests" href="how-to-write-tests.html" />
    <link rel="prev" title="4.5.2. Background" href="test-background.html" />
  <link rel="stylesheet" type="text/css"
    href="_static/ns3_stylesheet.css" />
    <link rel="icon" type="image/ico"
      href="_static/favicon.ico" />

  <script type="text/javascript" src="_static/drop-down-menu.js"></script>
  <script type="text/javascript" src="_static/ns3_version.js"></script>
  <script type="text/javascript">var ns3_builder="html";</script>
  <script type="text/javascript" src="_static/ns3_links.js"></script>


  </head><body>
    <div id="titlearea">
      <table cellspacing="0" cellpadding="0" width="100%">
        <tbody>
          <tr style="height: 56px;">
            <td id="projectlogo">
              <a id="ns3_home1"
                 href="http://www.nsnam.org/">
                 <img alt="ns-3 Logo" style="background-color: unset;"
                      src="_static/ns-3-inverted-notext-small.png"/>
              </a>
            </td>
            <td id="projecttext">
              <div id="projectbrief">A Discrete-Event Network Simulator</div>
              <span id="projectnumber"><script type="text/javascript">document.write(ns3_version)</script></span>
            </td>

            <td id="ns3-menu">
              <div class="menu">
                <ul >
                  <li style="background-image:none">
                    <a id="ns3_home2"
                         href="http://www.nsnam.org/"
                         >&nbsp;&nbsp;Home</a>
                  </li>
                  <li><span
                        onmouseover="mopen('mTuts')"
                        onmouseout="mclosetime()"
                          >Tutorials &nbsp;&#x25BC;</span>
                      <div id="mTuts"
                          onmouseover="mcancelclosetime()"
                          onmouseout="mclosetime()">
                        <a id="ns3_tut"
                           href="/docs/tutorial/html/index.html"
                            >English</a><br/>
                      </div>
                  </li>
                  <li><span
                        onmouseover="mopen('mDocs')"
                        onmouseout="mclosetime()"
                          >Documentation &nbsp;&#x25BC;</span>
                      <div id="mDocs"
                          onmouseover="mcancelclosetime()"
                          onmouseout="mclosetime()">
                        <a id="ns3_ins"
                           href="/docs/installation/html/index.html"
                           >Installation</a><br/>
                        <a id="ns3_man"
                           href="/docs/manual/html/index.html"
                           >Manual</a><br/>
                        <a id="ns3_mod"
                           href="/docs/models/html/index.html"
                           >Models</a><br/>
                        <a id="ns3_con"
                           href="/docs/contributing/html/index.html"
                           >Contributing</a><br/>
                        <a id="ns3_wiki"
                           href="http://www.nsnam.org/wiki"
                           >Wiki</a><br/>
                      </div>
                  </li>
                  <li><span
                        onmouseover="mopen('mDev')"
                        onmouseout="mclosetime()"
                          >Development &nbsp;&#x25BC;</span>
                      <div id="mDev"
                          onmouseover="mcancelclosetime()"
                          onmouseout="mclosetime()">
                        <a id="ns3_api"
                           href="/docs/doxygen/html/index.html"
                           >API Docs</a><br/>
                        <a id="ns3_bugs"
                         href="https://gitlab.com/nsnam/ns-3-dev/-/issues"
                           >Issue Tracker</a><br/>
                        <a id="ns3_merge"
                         href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests"
                           >Merge Requests</a><br/>
                      </div>
                  </li>
                </ul>
              </div>
            </td>

            <td id="projectsection">
              <span style="margin-right:10px">Manual</span>
            </td>
          </tr>
        </tbody>
      </table>
      <script  type="text/javascript">ns3_write_links()</script>
    </div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="how-to-write-tests.html" title="4.5.4. How to write tests"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="test-background.html" title="4.5.2. Background"
             accesskey="P">previous</a> |</li>
    <li class="navelem"><a href="">ns-3</a><span class="navelem">&nbsp;</span></li>
    
        <li class="nav-item nav-item-0"><a href="index.html">Manual</a><span class="navelem">&nbsp;</span></li>

          <li class="nav-item nav-item-1"><a href="develop.html" ><span class="section-number">4. </span>Developer Tools</a><span class="navelem">&nbsp;</span></li>
          <li class="nav-item nav-item-2"><a href="tests.html" accesskey="U"><span class="section-number">4.5. </span>Tests</a><span class="navelem">&nbsp;</span></li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4.5.3. </span>Testing framework</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="testing-framework">
<h1><span class="section-number">4.5.3. </span>Testing framework<a class="headerlink" href="#testing-framework" title="Link to this heading">¶</a></h1>
<p><em>ns-3</em> consists of a simulation core engine, a set of models, example programs,
and tests.  Over time, new contributors contribute models, tests, and
examples.  A Python test program <code class="docutils literal notranslate"><span class="pre">test.py</span></code> serves as the test
execution manager; <code class="docutils literal notranslate"><span class="pre">test.py</span></code> can run test code and examples to
look for regressions, can output the results into a number of forms, and
can manage code coverage analysis tools.  On top of this, we layer
<em>buildslaves</em> that are automated build robots that perform
robustness testing by running the test framework on different systems
and with different configuration options.</p>
<section id="buildslaves">
<h2><span class="section-number">4.5.3.1. </span>Buildslaves<a class="headerlink" href="#buildslaves" title="Link to this heading">¶</a></h2>
<p>At the highest level of <em>ns-3</em> testing are the buildslaves (build robots).
If you are unfamiliar with
this system look at <a class="reference external" href="https://ns-buildmaster.ee.washington.edu:8010/">https://ns-buildmaster.ee.washington.edu:8010/</a>.
This is an open-source automated system that allows <em>ns-3</em> to be rebuilt
and tested daily.  By running the buildbots on a number
of different systems we can ensure that <em>ns-3</em> builds and executes
properly on all of its supported systems.</p>
<p>Users (and developers) typically will not interact with the buildslave system other
than to read its messages regarding test results.  If a failure is detected in
one of the automated build and test jobs, the buildbot will send an email to the
<em>ns-commits</em> mailing list.  This email will look something like</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Ns-commits] Build failed in Jenkins: daily-ubuntu-without-valgrind » Ubuntu-64-15.04 #926

...
281 of 285 tests passed (281 passed, 3 skipped, 1 failed, 0 crashed, 0 valgrind errors)
List of SKIPped tests:
  ns3-tcp-cwnd
  ns3-tcp-interoperability
  nsc-tcp-loss
List of FAILed tests:
  random-variable-stream-generators
+ exit 1
Build step &#39;Execute shell&#39; marked build as failure
</pre></div>
</div>
<p>In the full details URL shown in the email, one can find links to the detailed test output.</p>
<p>The buildslave system will do its job quietly if there are no errors, and the
system will undergo build and test cycles every day to verify that all is well.</p>
</section>
<section id="test-py">
<h2><span class="section-number">4.5.3.2. </span>Test.py<a class="headerlink" href="#test-py" title="Link to this heading">¶</a></h2>
<p>The buildbots use a Python program, <code class="docutils literal notranslate"><span class="pre">test.py</span></code>, that is responsible for
running all of the tests and collecting the resulting reports into a human-
readable form.  This program is also available for use by users and developers
as well.</p>
<p><code class="docutils literal notranslate"><span class="pre">test.py</span></code> is very flexible in allowing the user to specify the number
and kind of tests to run; and also the amount and kind of output to generate.</p>
<p>Before running <code class="docutils literal notranslate"><span class="pre">test.py</span></code>, make sure that ns3’s examples and tests
have been built by doing the following</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>configure<span class="w"> </span>--enable-examples<span class="w"> </span>--enable-tests
$<span class="w"> </span>./ns3<span class="w"> </span>build
</pre></div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> will run all available tests and report status
back in a very concise form.  Running the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py
</pre></div>
</div>
<p>will result in a number of <code class="docutils literal notranslate"><span class="pre">PASS</span></code>, <code class="docutils literal notranslate"><span class="pre">FAIL</span></code>, <code class="docutils literal notranslate"><span class="pre">CRASH</span></code> or <code class="docutils literal notranslate"><span class="pre">SKIP</span></code>
indications followed by the kind of test that was run and its display name.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Waf: Entering directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
Waf: Leaving directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
&#39;build&#39; finished successfully (0.939s)
FAIL: TestSuite propagation-loss-model
PASS: TestSuite object-name-service
PASS: TestSuite pcap-file-object
PASS: TestSuite ns3-tcp-cwnd
...
PASS: TestSuite ns3-tcp-interoperability
PASS: Example csma-broadcast
PASS: Example csma-multicast
</pre></div>
</div>
<p>This mode is intended to be used by users who are interested in determining if
their distribution is working correctly, and by developers who are interested
in determining if changes they have made have caused any regressions.</p>
<p>There are a number of options available to control the behavior of <code class="docutils literal notranslate"><span class="pre">test.py</span></code>.
if you run <code class="docutils literal notranslate"><span class="pre">test.py</span> <span class="pre">--help</span></code> you should see a command summary like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Usage: test.py [options]

Options:
  -h, --help            show this help message and exit
  -b BUILDPATH, --buildpath=BUILDPATH
                        specify the path where ns-3 was built (defaults to the
                        build directory for the current variant)
  -c KIND, --constrain=KIND
                        constrain the test-runner by kind of test
  -e EXAMPLE, --example=EXAMPLE
                        specify a single example to run (no relative path is
                        needed)
  -d, --duration        print the duration of each test suite and example
  -e EXAMPLE, --example=EXAMPLE
                        specify a single example to run (no relative path is
                        needed)
  -u, --update-data     If examples use reference data files, get them to re-
                        generate them
  -f FULLNESS, --fullness=FULLNESS
                        choose the duration of tests to run: QUICK, EXTENSIVE,
                        or TAKES_FOREVER, where EXTENSIVE includes QUICK and
                        TAKES_FOREVER includes QUICK and EXTENSIVE (only QUICK
                        tests are run by default)
  -g, --grind           run the test suites and examples using valgrind
  -k, --kinds           print the kinds of tests available
  -l, --list            print the list of known tests
  -m, --multiple        report multiple failures from test suites and test
                        cases
  -n, --nobuild           do not run ns3 before starting testing
  -p PYEXAMPLE, --pyexample=PYEXAMPLE
                        specify a single python example to run (with relative
                        path)
  -r, --retain          retain all temporary files (which are normally
                        deleted)
  -s TEST-SUITE, --suite=TEST-SUITE
                        specify a single test suite to run
  -t TEXT-FILE, --text=TEXT-FILE
                        write detailed test results into TEXT-FILE.txt
  -v, --verbose         print progress and informational messages
  -w HTML-FILE, --web=HTML-FILE, --html=HTML-FILE
                        write detailed test results into HTML-FILE.html
  -x XML-FILE, --xml=XML-FILE
                        write detailed test results into XML-FILE.xml
</pre></div>
</div>
<p>If one specifies an optional output style, one can generate detailed descriptions
of the tests and status.  Available styles are <code class="docutils literal notranslate"><span class="pre">text</span></code> and <code class="docutils literal notranslate"><span class="pre">HTML</span></code>.
The buildbots will select the HTML option to generate HTML test reports for the
nightly builds using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--html<span class="o">=</span>nightly.html
</pre></div>
</div>
<p>In this case, an HTML file named ‘’nightly.html’’ would be created with a pretty
summary of the testing done.  A ‘’human readable’’ format is available for users
interested in the details.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--text<span class="o">=</span>results.txt
</pre></div>
</div>
<p>In the example above, the test suite checking the <em>ns-3</em> wireless
device propagation loss models failed.  By default no further information is
provided.</p>
<p>To further explore the failure, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> allows a single test suite
to be specified.  Running the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--suite<span class="o">=</span>propagation-loss-model
</pre></div>
</div>
<p>or equivalently</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>-s<span class="w"> </span>propagation-loss-model
</pre></div>
</div>
<p>results in that single test suite being run.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FAIL: TestSuite propagation-loss-model
</pre></div>
</div>
<p>To find detailed information regarding the failure, one must specify the kind
of output desired.  For example, most people will probably be interested in
a text file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--suite<span class="o">=</span>propagation-loss-model<span class="w"> </span>--text<span class="o">=</span>results.txt
</pre></div>
</div>
<p>This will result in that single test suite being run with the test status written to
the file ‘’results.txt’’.</p>
<p>You should find something similar to the following in that file</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FAIL: Test Suite &#39;&#39;propagation-loss-model&#39;&#39; (real 0.02 user 0.01 system 0.00)
PASS: Test Case &quot;Check ... Friis ... model ...&quot; (real 0.01 user 0.00 system 0.00)
FAIL: Test Case &quot;Check ... Log Distance ... model&quot; (real 0.01 user 0.01 system 0.00)
  Details:
    Message:   Got unexpected SNR value
    Condition: [long description of what actually failed]
    Actual:    176.395
    Limit:     176.407 +- 0.0005
    File:      ../src/test/ns3wifi/propagation-loss-models-test-suite.cc
    Line:      360
</pre></div>
</div>
<p>Notice that the Test Suite is composed of two Test Cases.  The first test case
checked the Friis propagation loss model and passed.  The second test case
failed checking the Log Distance propagation model.  In this case, an SNR of
176.395 was found, and the test expected a value of 176.407 correct to three
decimal places.  The file which implemented the failing test is listed as well
as the line of code which triggered the failure.</p>
<p>If you desire, you could just as easily have written an HTML file using the
<code class="docutils literal notranslate"><span class="pre">--html</span></code> option as described above.</p>
<p>Typically a user will run all tests at least once after downloading
<em>ns-3</em> to ensure that his or her environment has been built correctly
and is generating correct results according to the test suites.  Developers
will typically run the test suites before and after making a change to ensure
that they have not introduced a regression with their changes.  In this case,
developers may not want to run all tests, but only a subset.  For example,
the developer might only want to run the unit tests periodically while making
changes to a repository.  In this case, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> can be told to constrain
the types of tests being run to a particular class of tests.  The following
command will result in only the unit tests being run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--constrain<span class="o">=</span>unit
</pre></div>
</div>
<p>To see a quick list of the legal kinds of constraints, you can ask for them
to be listed.  The following command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--kinds
</pre></div>
</div>
<p>will result in the following list being displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Waf: Entering directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
Waf: Leaving directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
&#39;build&#39; finished successfully (0.939s)Waf: Entering directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
core:        Run all TestSuite-based tests (exclude examples)
example:     Examples (to see if example programs run successfully)
performance: Performance Tests (check to see if the system is as fast as expected)
system:      System Tests (spans modules to check integration of modules)
unit:        Unit Tests (within modules to check basic functionality)
</pre></div>
</div>
<p>Any of these kinds of test can be provided as a constraint using the <code class="docutils literal notranslate"><span class="pre">--constraint</span></code>
option.</p>
<p>To see a quick list of all of the test suites available, you can ask for them
to be listed.  The following command,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--list
</pre></div>
</div>
<p>will result in a list of the test suite being displayed, similar to</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Waf: Entering directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
Waf: Leaving directory `/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build&#39;
&#39;build&#39; finished successfully (0.939s)

Test Type    Test Name
---------    ---------
performance  many-uniform-random-variables-one-get-value-call
performance  one-uniform-random-variable-many-get-value-calls
performance  type-id-perf
system       buildings-pathloss-test
system       buildings-shadowing-test
system       devices-mesh-dot11s-regression
system       devices-mesh-flame-regression
system       epc-gtpu
...
unit         wimax-phy-layer
unit         wimax-service-flow
unit         wimax-ss-mac-layer
unit         wimax-tlv
example      adhoc-aloha-ideal-phy
example      adhoc-aloha-ideal-phy-matrix-propagation-loss-model
example      adhoc-aloha-ideal-phy-with-microwave-oven
example      aodv
...
</pre></div>
</div>
<p>Any of these listed suites can be selected to be run by itself using the
<code class="docutils literal notranslate"><span class="pre">--suite</span></code> option as shown above.</p>
<p>To run multiple test suites at once it is possible to use a ‘Unix filename pattern matching’
style, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>../test.py<span class="w"> </span>-s<span class="w"> </span><span class="s1">&#39;ipv6*&#39;</span>
</pre></div>
</div>
<p>Note the use of quotes. The result is similar to</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PASS: TestSuite ipv6-protocol
PASS: TestSuite ipv6-packet-info-tag
PASS: TestSuite ipv6-list-routing
PASS: TestSuite ipv6-extension-header
PASS: TestSuite ipv6-address-generator
PASS: TestSuite ipv6-raw
PASS: TestSuite ipv6-dual-stack
PASS: TestSuite ipv6-fragmentation
PASS: TestSuite ipv6-address-helper
PASS: TestSuite ipv6-address
PASS: TestSuite ipv6-forwarding
PASS: TestSuite ipv6-ripng
</pre></div>
</div>
<p>Similarly to test suites, one can run a single C++ example program
using the <code class="docutils literal notranslate"><span class="pre">--example</span></code> option.  Note that the relative path for the
example does not need to be included and that the executables built
for C++ examples do not have extensions.  Furthermore, the example
must be registered as an example to the test framework; it is not
sufficient to create an example and run it through test.py; it must
be added to the relevant <code class="docutils literal notranslate"><span class="pre">examples-to-run.py</span></code> file, explained below.
Entering</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--example<span class="o">=</span>udp-echo
</pre></div>
</div>
<p>results in that single example being run.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PASS: Example examples/udp/udp-echo
</pre></div>
</div>
<p>You can specify the directory where <em>ns-3</em> was built using the
<code class="docutils literal notranslate"><span class="pre">--buildpath</span></code> option as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--buildpath<span class="o">=</span>/home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build/debug<span class="w"> </span>--example<span class="o">=</span>wifi-simple-adhoc
</pre></div>
</div>
<p>One can run a single Python example program using the <code class="docutils literal notranslate"><span class="pre">--pyexample</span></code>
option.  Note that the relative path for the example must be included
and that Python examples do need their extensions.  Entering</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--pyexample<span class="o">=</span>examples/tutorial/first.py
</pre></div>
</div>
<p>results in that single example being run.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PASS: Example examples/tutorial/first.py
</pre></div>
</div>
<p>Because Python examples are not built, you do not need to specify the
directory where <em>ns-3</em> was built to run them.</p>
<p>Normally when example programs are executed, they write a large amount of trace
file data.  This is normally saved to the base directory of the distribution
(e.g., /home/user/ns-3-dev).  When <code class="docutils literal notranslate"><span class="pre">test.py</span></code> runs an example, it really
is completely unconcerned with the trace files.  It just wants to to determine
if the example can be built and run without error.  Since this is the case, the
trace files are written into a <code class="docutils literal notranslate"><span class="pre">/tmp/unchecked-traces</span></code> directory.  If you
run the above example, you should be able to find the associated
<code class="docutils literal notranslate"><span class="pre">udp-echo.tr</span></code> and <code class="docutils literal notranslate"><span class="pre">udp-echo-n-1.pcap</span></code> files there.</p>
<p>The list of available examples is defined by the contents of the ‘’examples’’
directory in the distribution.  If you select an example for execution using
the <code class="docutils literal notranslate"><span class="pre">--example</span></code> option, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> will not make any attempt to decide
if the example has been configured or not, it will just try to run it and
report the result of the attempt.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">test.py</span></code> runs, by default it will first ensure that the system has
been completely built.  This can be defeated by selecting the <code class="docutils literal notranslate"><span class="pre">--nobuild</span></code>
option.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--list<span class="w"> </span>--nobuild
</pre></div>
</div>
<p>will result in a list of the currently built test suites being displayed, similar to:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>propagation-loss-model
ns3-tcp-cwnd
ns3-tcp-interoperability
pcap-file
object-name-service
random-variable-stream-generators
</pre></div>
</div>
<p>Note the absence of the <code class="docutils literal notranslate"><span class="pre">ns3</span></code> build messages.</p>
<p><code class="docutils literal notranslate"><span class="pre">test.py</span></code> also supports running the test suites and examples under valgrind.
Valgrind is a flexible program for debugging and profiling Linux executables.  By
default, valgrind runs a tool called memcheck, which performs a range of memory-
checking functions, including detecting accesses to uninitialised memory, misuse
of allocated memory (double frees, access after free, etc.) and detecting memory
leaks.  This can be selected by using the <code class="docutils literal notranslate"><span class="pre">--grind</span></code> option.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--grind
</pre></div>
</div>
<p>As it runs, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> and the programs that it runs indirectly, generate large
numbers of temporary files.  Usually, the content of these files is not interesting,
however in some cases it can be useful (for debugging purposes) to view these files.
<code class="docutils literal notranslate"><span class="pre">test.py</span></code> provides a <code class="docutils literal notranslate"><span class="pre">--retain</span></code> option which will cause these temporary
files to be kept after the run is completed.  The files are saved in a directory
named <code class="docutils literal notranslate"><span class="pre">testpy-output</span></code> under a subdirectory named according to the current Coordinated
Universal Time (also known as Greenwich Mean Time).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--retain
</pre></div>
</div>
<p>Finally, <code class="docutils literal notranslate"><span class="pre">test.py</span></code> provides a <code class="docutils literal notranslate"><span class="pre">--verbose</span></code> option which will print
large amounts of information about its progress.  It is not expected that this
will be terribly useful unless there is an error.  In this case, you can get
access to the standard output and standard error reported by running test suites
and examples.  Select verbose in the following way:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--verbose
</pre></div>
</div>
<p>All of these options can be mixed and matched.  For example, to run all of the
<em>ns-3</em> core test suites under valgrind, in verbose mode, while generating an HTML
output file, one would do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>--verbose<span class="w"> </span>--grind<span class="w"> </span>--constrain<span class="o">=</span>core<span class="w"> </span>--html<span class="o">=</span>results.html
</pre></div>
</div>
</section>
<section id="testtaxonomy">
<h2><span class="section-number">4.5.3.3. </span>TestTaxonomy<a class="headerlink" href="#testtaxonomy" title="Link to this heading">¶</a></h2>
<p>As mentioned above, tests are grouped into a number of broadly defined
classifications to allow users to selectively run tests to address the different
kinds of testing that need to be done.</p>
<ul class="simple">
<li><p>Build Verification Tests</p></li>
<li><p>Unit Tests</p></li>
<li><p>System Tests</p></li>
<li><p>Examples</p></li>
<li><p>Performance Tests</p></li>
</ul>
<p>Moreover, each test is further classified according to the expected time needed to
run it. Tests are classified as:</p>
<ul class="simple">
<li><p>QUICK</p></li>
<li><p>EXTENSIVE</p></li>
<li><p>TAKES_FOREVER</p></li>
</ul>
<p>Note that specifying EXTENSIVE fullness will also run tests in QUICK category.
Specifying TAKES_FOREVER will run tests in EXTENSIVE and QUICK categories.
By default, only QUICK tests are ran.</p>
<p>As a rule of thumb, tests that must be run to ensure <em>ns-3</em> coherence should be
QUICK (i.e., take a few seconds). Tests that could be skipped, but are nice to do
can be EXTENSIVE; these are tests that typically need minutes. TAKES_FOREVER is
left for tests that take a really long time, in the order of several minutes.
The main classification goal is to be able to run the buildbots in a reasonable
time, and still be able to perform more extensive tests when needed.</p>
<section id="unit-tests">
<h3><span class="section-number">4.5.3.3.1. </span>Unit Tests<a class="headerlink" href="#unit-tests" title="Link to this heading">¶</a></h3>
<p>Unit tests are more involved tests that go into detail to make sure that a
piece of code works as advertised in isolation.  There is really no reason
for this kind of test to be built into an <em>ns-3</em> module.  It turns out, for
example, that the unit tests for the object name service are about the same
size as the object name service code itself.  Unit tests are tests that
check a single bit of functionality that are not built into the <em>ns-3</em> code,
but live in the same directory as the code it tests.  It is possible that
these tests check integration of multiple implementation files in a module
as well.  The file src/core/test/names-test-suite.cc is an example of this kind
of test.  The file src/network/test/pcap-file-test-suite.cc is another example
that uses a known good pcap file as a test vector file.  This file is stored
locally in the src/network directory.</p>
</section>
<section id="system-tests">
<h3><span class="section-number">4.5.3.3.2. </span>System Tests<a class="headerlink" href="#system-tests" title="Link to this heading">¶</a></h3>
<p>System tests are those that involve more than one module in the system.  We
have some of this kind of test running in our current regression framework,
but they are typically overloaded examples.  We provide a new place
for this kind of test in the directory <code class="docutils literal notranslate"><span class="pre">src/test</span></code>.  The file
<code class="docutils literal notranslate"><span class="pre">src/test/ns3tcp/ns3tcp-loss-test-suite.cc</span></code> is an example of this kind of
test.  It uses NSC TCP to test the <em>ns-3</em> TCP implementation.  Often there
will be test vectors required for this kind of test, and they are stored in
the directory where the test lives.  For example,
<code class="docutils literal notranslate"><span class="pre">ns3tcp-loss-NewReno0-response-vectors.pcap</span></code> is a file consisting of a number of TCP
headers that are used as the expected responses of the <em>ns-3</em> TCP under test.</p>
<p>Note that Unit Tests are often preferable to System Tests, as they are more
independent from small changes in the modules that are not the goal of the test.</p>
</section>
<section id="examples">
<h3><span class="section-number">4.5.3.3.3. </span>Examples<a class="headerlink" href="#examples" title="Link to this heading">¶</a></h3>
<p>The examples are tested by the framework to make sure they built and
will run.  Limited checking is done on examples; currently the pcap
files are just written off into /tmp to be discarded.  If the example
runs (don’t crash) and the exit status is zero, the example will pass
the smoke test.</p>
</section>
<section id="performance-tests">
<h3><span class="section-number">4.5.3.3.4. </span>Performance Tests<a class="headerlink" href="#performance-tests" title="Link to this heading">¶</a></h3>
<p>Performance tests are those which exercise a particular part of the system
and determine if the tests have executed to completion in a reasonable time.</p>
</section>
</section>
<section id="running-tests">
<h2><span class="section-number">4.5.3.4. </span>Running Tests<a class="headerlink" href="#running-tests" title="Link to this heading">¶</a></h2>
<p>Tests are typically run using the high level <code class="docutils literal notranslate"><span class="pre">test.py</span></code> program. To get a list of the available command-line options, run <code class="docutils literal notranslate"><span class="pre">test.py</span> <span class="pre">--help</span></code></p>
<p>The test program <code class="docutils literal notranslate"><span class="pre">test.py</span></code> will run both tests and those examples that
have been added to the list to check.  The difference between tests
and examples is as follows.  Tests generally check that specific simulation
output or events conforms to expected behavior.  In contrast, the output
of examples is not checked, and the test program merely checks the exit
status of the example program to make sure that it runs without error.</p>
<p>Briefly, to run all tests, first one must configure tests during configuration
stage, and also (optionally) examples if examples are to be checked:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>configure<span class="w"> </span>--enable-examples<span class="w"> </span>--enable-tests
</pre></div>
</div>
<p>Then, build <em>ns-3</em>, and after it is built, just run <code class="docutils literal notranslate"><span class="pre">test.py</span></code>.  <code class="docutils literal notranslate"><span class="pre">test.py</span> <span class="pre">-h</span></code>
will show a number of configuration options that modify the behavior
of test.py.</p>
<p>The program <code class="docutils literal notranslate"><span class="pre">test.py</span></code> invokes, for C++ tests and examples, a lower-level
C++ program called <code class="docutils literal notranslate"><span class="pre">test-runner</span></code> to actually run the tests.  As discussed
below, this <code class="docutils literal notranslate"><span class="pre">test-runner</span></code> can be a helpful way to debug tests.</p>
</section>
<section id="debugging-tests">
<h2><span class="section-number">4.5.3.5. </span>Debugging Tests<a class="headerlink" href="#debugging-tests" title="Link to this heading">¶</a></h2>
<p>The debugging of the test programs is best performed running the low-level
test-runner program. The test-runner is the bridge from generic Python
code to <em>ns-3</em> code. It is written in C++ and uses the automatic test
discovery process in the <em>ns-3</em> code to find and allow execution of all
of the various tests.</p>
<p>The main reason why <code class="docutils literal notranslate"><span class="pre">test.py</span></code> is not suitable for debugging is that it is
not allowed for logging to be turned on using the <code class="docutils literal notranslate"><span class="pre">NS_LOG</span></code> environmental
variable when test.py runs.  This limitation does not apply to the test-runner
executable. Hence, if you want to see logging output from your tests, you
have to run them using the test-runner directly.</p>
<p>In order to execute the test-runner, you run it like any other <em>ns-3</em> executable
– using <code class="docutils literal notranslate"><span class="pre">ns3</span></code>.  To get a list of available options, you can type:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span><span class="s2">&quot;test-runner --help&quot;</span>
</pre></div>
</div>
<p>You should see something like the following</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Usage: /home/craigdo/repos/ns-3-allinone-test/ns-3-dev/build/utils/ns3-dev-test-runner-debug [OPTIONS]

Options:
--help                 : print these options
--print-test-name-list : print the list of names of tests available
--list                 : an alias for --print-test-name-list
--print-test-types     : print the type of tests along with their names
--print-test-type-list : print the list of types of tests available
--print-temp-dir       : print name of temporary directory before running
                         the tests
--test-type=TYPE       : process only tests of type TYPE
--test-name=NAME       : process only test whose name matches NAME
--suite=NAME           : an alias (here for compatibility reasons only)
                         for --test-name=NAME
--assert-on-failure    : when a test fails, crash immediately (useful
                         when running under a debugger
--stop-on-failure      : when a test fails, stop immediately
--fullness=FULLNESS    : choose the duration of tests to run: QUICK,
                         EXTENSIVE, or TAKES_FOREVER, where EXTENSIVE
                         includes QUICK and TAKES_FOREVER includes
                         QUICK and EXTENSIVE (only QUICK tests are
                         run by default)
--verbose              : print details of test execution
--xml                  : format test run output as xml
--tempdir=DIR          : set temp dir for tests to store output files
--datadir=DIR          : set data dir for tests to read reference files
--out=FILE             : send test result to FILE instead of standard output
--append=FILE          : append test result to FILE instead of standard output
</pre></div>
</div>
<p>There are a number of things available to you which will be familiar to you if
you have looked at <code class="docutils literal notranslate"><span class="pre">test.py</span></code>.  This should be expected since the test-
runner is just an interface between <code class="docutils literal notranslate"><span class="pre">test.py</span></code> and <em>ns-3</em>.  You
may notice that example-related commands are missing here.  That is because
the examples are really not <em>ns-3</em> tests.  <code class="docutils literal notranslate"><span class="pre">test.py</span></code> runs them
as if they were to present a unified testing environment, but they are really
completely different and not to be found here.</p>
<p>The first new option that appears here, but not in test.py is the <code class="docutils literal notranslate"><span class="pre">--assert-on-failure</span></code>
option.  This option is useful when debugging a test case when running under a
debugger like <code class="docutils literal notranslate"><span class="pre">gdb</span></code>.  When selected, this option tells the underlying
test case to cause a segmentation violation if an error is detected.  This has
the nice side-effect of causing program execution to stop (break into the
debugger) when an error is detected.  If you are using gdb, you could use this
option something like,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>shell
$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build/utils
$<span class="w"> </span>gdb<span class="w"> </span>ns3-dev-test-runner-debug
$<span class="w"> </span>run<span class="w"> </span>--suite<span class="o">=</span>global-value<span class="w"> </span>--assert-on-failure
</pre></div>
</div>
<p>If an error is then found in the global-value test suite, a segfault would be
generated and the (source level) debugger would stop at the <code class="docutils literal notranslate"><span class="pre">NS_TEST_ASSERT_MSG</span></code>
that detected the error.</p>
<p>To run one of the tests directly from the test-runner
using <code class="docutils literal notranslate"><span class="pre">ns3</span></code>, you will need to specify the test suite to run.
So you could use the shell and do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span><span class="s2">&quot;test-runner --suite=pcap-file&quot;</span>
</pre></div>
</div>
<p><em>ns-3</em> logging is available when you run it this way, such as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">NS_LOG</span><span class="o">=</span><span class="s2">&quot;Packet&quot;</span><span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span><span class="s2">&quot;test-runner --suite=pcap-file&quot;</span>
</pre></div>
</div>
<section id="test-output">
<h3><span class="section-number">4.5.3.5.1. </span>Test output<a class="headerlink" href="#test-output" title="Link to this heading">¶</a></h3>
<p>Many test suites need to write temporary files (such as pcap files)
in the process of running the tests.  The tests then need a temporary directory
to write to.  The Python test utility (test.py) will provide a temporary file
automatically, but if run stand-alone this temporary directory must be provided.
It can be annoying to continually have to provide
a <code class="docutils literal notranslate"><span class="pre">--tempdir</span></code>, so the test runner will figure one out for you if you don’t
provide one.  It first looks for environment variables named <code class="docutils literal notranslate"><span class="pre">TMP</span></code> and
<code class="docutils literal notranslate"><span class="pre">TEMP</span></code> and uses those.  If neither <code class="docutils literal notranslate"><span class="pre">TMP</span></code> nor <code class="docutils literal notranslate"><span class="pre">TEMP</span></code> are defined
it picks <code class="docutils literal notranslate"><span class="pre">/tmp</span></code>.  The code then tacks on an identifier indicating what
created the directory (ns-3) then the time (hh.mm.ss) followed by a large random
number.  The test runner creates a directory of that name to be used as the
temporary directory.  Temporary files then go into a directory that will be
named something like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/tmp/ns-3.10.25.37.61537845
</pre></div>
</div>
<p>The time is provided as a hint so that you can relatively easily reconstruct
what directory was used if you need to go back and look at the files that were
placed in that directory.</p>
<p>Another class of output is test output like pcap traces that are generated
to compare to reference output.  The test program will typically delete
these after the test suites all run.  To disable the deletion of test
output, run <code class="docutils literal notranslate"><span class="pre">test.py</span></code> with the “retain” option:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./test.py<span class="w"> </span>-r
</pre></div>
</div>
<p>and test output can be found in the <code class="docutils literal notranslate"><span class="pre">testpy-output/</span></code> directory.</p>
</section>
<section id="reporting-of-test-failures">
<h3><span class="section-number">4.5.3.5.2. </span>Reporting of test failures<a class="headerlink" href="#reporting-of-test-failures" title="Link to this heading">¶</a></h3>
<p>When you run a test suite using the test-runner it will run the test
and report PASS or FAIL.
To run more quietly, you need to specify an output file to which the tests will write their status using the <code class="docutils literal notranslate"><span class="pre">--out</span></code> option.
Try,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span><span class="s2">&quot;test-runner --suite=pcap-file --out=myfile.txt&quot;</span>
</pre></div>
</div>
</section>
<section id="debugging-test-suite-failures">
<h3><span class="section-number">4.5.3.5.3. </span>Debugging test suite failures<a class="headerlink" href="#debugging-test-suite-failures" title="Link to this heading">¶</a></h3>
<p>To debug test crashes, such as</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CRASH: TestSuite wifi-interference
</pre></div>
</div>
<p>You can access the underlying test-runner program via gdb as follows, and
then pass the “–basedir=`pwd`” argument to run (you can also pass other
arguments as needed, but basedir is the minimum needed):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span><span class="s2">&quot;test-runner&quot;</span><span class="w"> </span>--command-template<span class="o">=</span><span class="s2">&quot;gdb %s&quot;</span>
Waf:<span class="w"> </span>Entering<span class="w"> </span>directory<span class="w"> </span><span class="sb">`</span>/home/tomh/hg/sep09/ns-3-allinone/ns-3-dev-678/build<span class="s1">&#39;</span>
<span class="s1">Waf: Leaving directory `/home/tomh/hg/sep09/ns-3-allinone/ns-3-dev-678/build&#39;</span>
<span class="s1">&#39;build&#39;</span><span class="w"> </span>finished<span class="w"> </span>successfully<span class="w"> </span><span class="o">(</span><span class="m">0</span>.380s<span class="o">)</span>
GNU<span class="w"> </span>gdb<span class="w"> </span><span class="m">6</span>.8-debian
Copyright<span class="w"> </span><span class="o">(</span>C<span class="o">)</span><span class="w"> </span><span class="m">2008</span><span class="w"> </span>Free<span class="w"> </span>Software<span class="w"> </span>Foundation,<span class="w"> </span>Inc.
L<span class="w"> </span>cense<span class="w"> </span>GPLv3+:<span class="w"> </span>GNU<span class="w"> </span>GPL<span class="w"> </span>version<span class="w"> </span><span class="m">3</span><span class="w"> </span>or<span class="w"> </span>later<span class="w"> </span>&lt;http://gnu.org/licenses/gpl.html&gt;
This<span class="w"> </span>is<span class="w"> </span>free<span class="w"> </span>software:<span class="w"> </span>you<span class="w"> </span>are<span class="w"> </span>free<span class="w"> </span>to<span class="w"> </span>change<span class="w"> </span>and<span class="w"> </span>redistribute<span class="w"> </span>it.
There<span class="w"> </span>is<span class="w"> </span>NO<span class="w"> </span>WARRANTY,<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>extent<span class="w"> </span>permitted<span class="w"> </span>by<span class="w"> </span>law.<span class="w">  </span>Type<span class="w"> </span><span class="s2">&quot;show copying&quot;</span>
and<span class="w"> </span><span class="s2">&quot;show warranty&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>details.
This<span class="w"> </span>GDB<span class="w"> </span>was<span class="w"> </span>configured<span class="w"> </span>as<span class="w"> </span><span class="s2">&quot;x86_64-linux-gnu&quot;</span>...
<span class="o">(</span>gdb<span class="o">)</span><span class="w"> </span>r<span class="w"> </span>--suite<span class="o">=</span>
Starting<span class="w"> </span>program:<span class="w"> </span>&lt;..&gt;/build/utils/ns3-dev-test-runner-debug<span class="w"> </span>--suite<span class="o">=</span>wifi-interference
<span class="o">[</span>Thread<span class="w"> </span>debugging<span class="w"> </span>using<span class="w"> </span>libthread_db<span class="w"> </span>enabled<span class="o">]</span>
assert<span class="w"> </span>failed.<span class="w"> </span><span class="nv">file</span><span class="o">=</span>../src/core/model/type-id.cc,<span class="w"> </span><span class="nv">line</span><span class="o">=</span><span class="m">138</span>,<span class="w"> </span><span class="nv">cond</span><span class="o">=</span><span class="s2">&quot;uid &lt;= m_information.size() &amp;&amp; uid != 0&quot;</span>
...
</pre></div>
</div>
<p>Here is another example of how to use valgrind to debug a memory problem
such as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>VALGR:<span class="w"> </span>TestSuite<span class="w"> </span>devices-mesh-dot11s-regression

$<span class="w"> </span>./ns3<span class="w"> </span>run<span class="w"> </span>test-runner<span class="w"> </span>--command-template<span class="o">=</span><span class="s2">&quot;valgrind %s --suite=devices-mesh-dot11s-regression&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="class-testrunner">
<h2><span class="section-number">4.5.3.6. </span>Class TestRunner<a class="headerlink" href="#class-testrunner" title="Link to this heading">¶</a></h2>
<p>The executables that run dedicated test programs use a TestRunner class.  This
class provides for automatic test registration and listing, as well as a way to
execute the individual tests.  Individual test suites use C++ global
constructors
to add themselves to a collection of test suites managed by the test runner.
The test runner is used to list all of the available tests and to select a test
to be run.  This is a quite simple class that provides three static methods to
provide or Adding and Getting test suites to a collection of tests.  See the
doxygen for class <code class="docutils literal notranslate"><span class="pre">ns3::TestRunner</span></code> for details.</p>
</section>
<section id="test-suite">
<h2><span class="section-number">4.5.3.7. </span>Test Suite<a class="headerlink" href="#test-suite" title="Link to this heading">¶</a></h2>
<p>All <em>ns-3</em> tests are classified into Test Suites and Test Cases.  A
test suite is a collection of test cases that completely exercise a given kind
of functionality.  As described above, test suites can be classified as,</p>
<ul class="simple">
<li><p>Build Verification Tests</p></li>
<li><p>Unit Tests</p></li>
<li><p>System Tests</p></li>
<li><p>Examples</p></li>
<li><p>Performance Tests</p></li>
</ul>
<p>This classification is exported from the TestSuite class.  This class is quite
simple, existing only as a place to export this type and to accumulate test
cases.  From a user perspective, in order to create a new TestSuite in the
system one only has to define a new class that inherits from class <code class="docutils literal notranslate"><span class="pre">TestSuite</span></code>
and perform these two duties.</p>
<p>The following code will define a new class that can be run by <code class="docutils literal notranslate"><span class="pre">test.py</span></code>
as a ‘’unit’’ test with the display name, <code class="docutils literal notranslate"><span class="pre">my-test-suite-name</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MySuite</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">TestSuite</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">MyTestSuite</span><span class="p">();</span>
<span class="p">};</span>

<span class="n">MyTestSuite</span><span class="o">::</span><span class="n">MyTestSuite</span><span class="p">()</span>
<span class="w">  </span><span class="o">:</span><span class="w"> </span><span class="n">TestSuite</span><span class="p">(</span><span class="s">&quot;my-test-suite-name&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">UNIT</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">AddTestCase</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">MyTestCase</span><span class="p">,</span><span class="w"> </span><span class="n">TestCase</span><span class="o">::</span><span class="n">QUICK</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span><span class="w"> </span><span class="n">MyTestSuite</span><span class="w"> </span><span class="n">myTestSuite</span><span class="p">;</span>
</pre></div>
</div>
<p>The base class takes care of all of the registration and reporting required to
be a good citizen in the test framework.</p>
<p>Avoid putting initialization logic into the test suite or test case
constructors.  This is
because an instance of the test suite is created at run time
(due to the static variable above) regardless of whether the test is being
run or not.  Instead, the TestCase provides a virtual <code class="docutils literal notranslate"><span class="pre">DoSetup</span></code> method
that can be specialized to perform setup before <code class="docutils literal notranslate"><span class="pre">DoRun</span></code> is called.</p>
</section>
<section id="test-case">
<h2><span class="section-number">4.5.3.8. </span>Test Case<a class="headerlink" href="#test-case" title="Link to this heading">¶</a></h2>
<p>Individual tests are created using a TestCase class.  Common models for the use
of a test case include “one test case per feature”, and “one test case per method.”
Mixtures of these models may be used.</p>
<p>In order to create a new test case in the system, all one has to do is to inherit
from the  <code class="docutils literal notranslate"><span class="pre">TestCase</span></code> base class, override the constructor to give the test
case a name and override the <code class="docutils literal notranslate"><span class="pre">DoRun</span></code> method to run the test.  Optionally,
override also the <code class="docutils literal notranslate"><span class="pre">DoSetup</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyTestCase</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">TestCase</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">MyTestCase</span><span class="p">();</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">DoSetup</span><span class="p">();</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">DoRun</span><span class="p">();</span>
<span class="p">};</span>

<span class="n">MyTestCase</span><span class="o">::</span><span class="n">MyTestCase</span><span class="p">()</span>
<span class="w">  </span><span class="o">:</span><span class="w"> </span><span class="n">TestCase</span><span class="p">(</span><span class="s">&quot;Check some bit of functionality&quot;</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">}</span>

<span class="kt">void</span>
<span class="n">MyTestCase</span><span class="o">::</span><span class="n">DoRun</span><span class="p">()</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">NS_TEST_ASSERT_MSG_EQ</span><span class="p">(</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Some failure message&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="utilities">
<h2><span class="section-number">4.5.3.9. </span>Utilities<a class="headerlink" href="#utilities" title="Link to this heading">¶</a></h2>
<p>There are a number of utilities of various kinds that are also part of the
testing framework.  Examples include a generalized pcap file useful for
storing test vectors; a generic container useful for transient storage of
test vectors during test execution; and tools for generating presentations
based on validation and verification testing results.</p>
<p>These utilities are not documented here, but for example, please see
how the TCP tests found in <code class="docutils literal notranslate"><span class="pre">src/test/ns3tcp/</span></code> use pcap files and reference
output.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">4.5.3. Testing framework</a><ul>
<li><a class="reference internal" href="#buildslaves">4.5.3.1. Buildslaves</a></li>
<li><a class="reference internal" href="#test-py">4.5.3.2. Test.py</a></li>
<li><a class="reference internal" href="#testtaxonomy">4.5.3.3. TestTaxonomy</a><ul>
<li><a class="reference internal" href="#unit-tests">4.5.3.3.1. Unit Tests</a></li>
<li><a class="reference internal" href="#system-tests">4.5.3.3.2. System Tests</a></li>
<li><a class="reference internal" href="#examples">4.5.3.3.3. Examples</a></li>
<li><a class="reference internal" href="#performance-tests">4.5.3.3.4. Performance Tests</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-tests">4.5.3.4. Running Tests</a></li>
<li><a class="reference internal" href="#debugging-tests">4.5.3.5. Debugging Tests</a><ul>
<li><a class="reference internal" href="#test-output">4.5.3.5.1. Test output</a></li>
<li><a class="reference internal" href="#reporting-of-test-failures">4.5.3.5.2. Reporting of test failures</a></li>
<li><a class="reference internal" href="#debugging-test-suite-failures">4.5.3.5.3. Debugging test suite failures</a></li>
</ul>
</li>
<li><a class="reference internal" href="#class-testrunner">4.5.3.6. Class TestRunner</a></li>
<li><a class="reference internal" href="#test-suite">4.5.3.7. Test Suite</a></li>
<li><a class="reference internal" href="#test-case">4.5.3.8. Test Case</a></li>
<li><a class="reference internal" href="#utilities">4.5.3.9. Utilities</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="test-background.html"
                          title="previous chapter"><span class="section-number">4.5.2. </span>Background</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="how-to-write-tests.html"
                          title="next chapter"><span class="section-number">4.5.4. </span>How to write tests</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/test-framework.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="how-to-write-tests.html" title="4.5.4. How to write tests"
             >next</a> |</li>
        <li class="right" >
          <a href="test-background.html" title="4.5.2. Background"
             >previous</a> |</li>
    <li class="navelem"><a href="">ns-3</a><span class="navelem">&nbsp;</span></li>
    
        <li class="nav-item nav-item-0"><a href="index.html">Manual</a><span class="navelem">&nbsp;</span></li>

          <li class="nav-item nav-item-1"><a href="develop.html" ><span class="section-number">4. </span>Developer Tools</a><span class="navelem">&nbsp;</span></li>
          <li class="nav-item nav-item-2"><a href="tests.html" ><span class="section-number">4.5. </span>Tests</a><span class="navelem">&nbsp;</span></li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4.5.3. </span>Testing framework</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2006-2019.
      Last updated on Feb 05, 2024 08:55.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>